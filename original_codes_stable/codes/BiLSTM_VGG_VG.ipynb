{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "from utils import *\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,log_device_placement=True,allow_soft_placement=True)\n",
    "import lmdb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "data_path = '/dvmm-filer2/datasets/Groundings/data/visualgenome/'\n",
    "dict_paths = [data_path+'train.pickle',\n",
    "              data_path+'val.pickle'] \n",
    "lmdb_path = data_path+'data.lmdb'\n",
    "\n",
    "#loading mscoco data\n",
    "lmdb_env = lmdb.open(lmdb_path, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn = lmdb_env.begin(write=False)\n",
    "\n",
    "with open(dict_paths[0], 'rb') as f:\n",
    "    dict_train = pickle.load(f, encoding='latin1')\n",
    "    ids_train = list(dict_train.keys())\n",
    "    \n",
    "ref_path = '/dvmm-filer2/datasets/Groundings/data/referit/referit_splits/'\n",
    "ref_lmdb_path = '/dvmm-filer2/datasets/Groundings/data/referit/refclef_data/all_data.lmdb'\n",
    "ref_dict_path = ref_path+'val.pickle' \n",
    "\n",
    "#loading referit data\n",
    "with open(ref_dict_path, 'rb') as f:\n",
    "    dict_val = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "ref_lmdb_env = lmdb.open(ref_lmdb_path, map_size=int(1e11), readonly=True, lock=False)\n",
    "ref_txn = ref_lmdb_env.begin(write=False)\n",
    "\n",
    "n_batch = 32\n",
    "gamma_1 = 5.0\n",
    "gamma_2 = 10.0\n",
    "n_iter_per_epoch = int(len(dict_train)/n_batch)\n",
    "n_iter_per_epoch_val = int(len(dict_val)/n_batch)\n",
    "n_epochs = 20 #N of epochs\n",
    "reg_val = .0005\n",
    "num_tst = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_referit(dict_test,txn):\n",
    "    cnt_overall = 0\n",
    "    cnt_correct_w = 0\n",
    "    cnt_correct_hit_w = 0\n",
    "    cnt_correct_s = 0\n",
    "    cnt_correct_hit_s = 0\n",
    "    for k,doc_id in enumerate(dict_test):\n",
    "        if k>num_tst:\n",
    "            continue\n",
    "        imgbin = txn.get(doc_id.encode('utf-8'))\n",
    "        if imgbin==None:\n",
    "            print (\"Image not found\")\n",
    "            continue\n",
    "        buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "        if len(buff) == 0:\n",
    "            print (\"Image not found\")\n",
    "            continue\n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        imgrgb = imgbgr[:,:,[2,1,0]]\n",
    "\n",
    "        img = np.reshape(cv2.resize(imgrgb,(299,299)),(1,299,299,3))\n",
    "        orig_img_shape = dict_test[doc_id]['size'][:2]\n",
    "\n",
    "        for i,annot in enumerate(dict_test[doc_id]['annotations']):\n",
    "            if len(annot['bbox_norm'])== 0:\n",
    "                continue\n",
    "            if not check_percent(union(annot['bbox_norm'])):\n",
    "                continue\n",
    "            if any(b>1 for b in annot['bbox_norm']):\n",
    "                continue\n",
    "            unq_qry = set(annot['query'])\n",
    "            sen_batch = [sen for sen in unq_qry if 0<len(sen.split())<=50] #only unique queries with 0<length<=50\n",
    "            img_batch = np.repeat(img,len(sen_batch),axis=0)\n",
    "            tensor_list = [heatmap_w, heatmap_s, R_i, R_s]\n",
    "            feed_dict = {input_img: img_batch, text_batch: sen_batch, mode: 'test'}\n",
    "            qry_heats, qry_heat, qry_scores, sen_score = sess.run(tensor_list, feed_dict)\n",
    "            # add length of unique queries\n",
    "            cnt_overall += len(sen_batch)\n",
    "            for c,sen in enumerate(sen_batch):\n",
    "                idx = [j for j in range(len(sen.split()))]\n",
    "                if np.mean(qry_scores[c,idx])==0:\n",
    "                    pred = {}\n",
    "                else:\n",
    "                    heatmap_wrd = np.average(qry_heats[c,idx,:], weights = qry_scores[c,idx], axis=0)\n",
    "                    heatmap_sen = qry_heat[c,:]\n",
    "                    bbox_c_w,hit_c_w = calc_correctness(annot,heatmap_wrd,orig_img_shape)\n",
    "                    bbox_c_s,hit_c_s = calc_correctness(annot,heatmap_sen,orig_img_shape)\n",
    "                    cnt_correct_w+=bbox_c_w\n",
    "                    cnt_correct_hit_w+=hit_c_w\n",
    "                    cnt_correct_s+=bbox_c_s\n",
    "                    cnt_correct_hit_s+=hit_c_s\n",
    "\n",
    "        var = [k,num_tst,cnt_correct_w/cnt_overall,cnt_correct_hit_w/cnt_overall]\n",
    "        var_s = [cnt_correct_s/cnt_overall,cnt_correct_hit_s/cnt_overall]\n",
    "        prnt0 = 'Sample {}/{}, IoU_acc_w:{:.2f}, IoU_acc_s:{:.2f}'.format(var[0],var[1],var[2],var_s[0])\n",
    "        prnt1 = ', Hit_acc_w:{:.2f}, Hit_acc_s:{:.2f} \\r'.format(var[3],var_s[1])\n",
    "        sys.stdout.write(prnt0+prnt1)                \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    hit_acc_w = cnt_correct_hit_w/cnt_overall\n",
    "    iou_acc_w = cnt_correct_w/cnt_overall\n",
    "    hit_acc_s = cnt_correct_hit_s/cnt_overall\n",
    "    iou_acc_s = cnt_correct_s/cnt_overall\n",
    "    \n",
    "    return iou_acc_w,hit_acc_w,iou_acc_s,hit_acc_s\n",
    "\n",
    "def batch_gen(ids, annot_dict, txn):\n",
    "    img_batch = np.empty((n_batch, 299, 299, 3), dtype='float32')\n",
    "    cap_batch = []\n",
    "    #currently, it takes negative samples randomly from \"all\" dataset\n",
    "    #it randomly picks any batch, so it doesn't have ending\n",
    "    seen = {}\n",
    "    for i in range(n_batch):\n",
    "        choice_id = random.choice(ids)\n",
    "        while choice_id in seen: #we don't want to have repetitive img/caps in a batch \n",
    "            choice_id = random.choice(ids)\n",
    "        imgbin = txn.get(choice_id.encode('utf-8'))\n",
    "        if imgbin!=None:\n",
    "            buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "        else:\n",
    "            buff = []\n",
    "        while choice_id in seen or len(buff)==0:\n",
    "            choice_id = random.choice(ids)\n",
    "            imgbin = txn.get(choice_id.encode('utf-8'))\n",
    "            if imgbin!=None:\n",
    "                buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "            else:\n",
    "                buff = []\n",
    "        seen[choice_id] = 1\n",
    "        \n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        img = imgbgr[:,:,[2,1,0]]\n",
    "        img_batch[i,:,:,:] = cv2.resize(img,(299,299))\n",
    "        queries = [annot['query'] for annot in dict_train[choice_id]['annotations']]\n",
    "        sentence = random.choice(queries)\n",
    "        cap_batch.append(sentence)\n",
    "    return img_batch, cap_batch\n",
    "\n",
    "def attn_loss(e_w,v,e_s):\n",
    "    #e: ?xTxD, v: ?xNx4xD, e_bar: ?xD\n",
    "    with tf.variable_scope('attention_loss'):\n",
    "        ###word-level###\n",
    "        #heatmap\n",
    "        h = tf.nn.relu(tf.einsum('bij,cklj->bcikl',e_w,v)) #pair-wise ev^T: ?x?xTxNx4\n",
    "        #attention\n",
    "        a = tf.einsum('bcijl,cjlk->bcikl',h,v) #?x?xTxDx4 attnded visual reps for each of T words for all pairs\n",
    "        #pair-wise score\n",
    "        a_norm = tf.nn.l2_normalize(a,axis=3)\n",
    "        e_w_norm = tf.nn.l2_normalize(e_w,axis=2)\n",
    "        R_ik = tf.einsum('bcilk,bil->bcik',a_norm,e_w_norm) #cosine for T (words,img_reps) for all pairs\n",
    "        #level dropout\n",
    "        #R_ik_sh = R_ik.get_shape().as_list()\n",
    "        #R_ik = tf.layers.dropout(R_ik,rate=0.5,noise_shape=[1,1,1,R_ik_sh[3]],\n",
    "        #                         training=isTraining)\n",
    "        R_i = tf.reduce_max(R_ik,axis=-1) #?x?xT\n",
    "        R = tf.log(tf.pow(tf.reduce_sum(tf.exp(gamma_1*R_i),axis=2),1/gamma_1)) #?x? cap-img pairs\n",
    "        #posterior probabilities\n",
    "        P_DQ = tf.diag_part(tf.nn.softmax(gamma_2*R,axis=0)) #P(cap match img)\n",
    "        P_QD = tf.diag_part(tf.nn.softmax(gamma_2*R,axis=1)) #p(img match cap)\n",
    "        #losses\n",
    "        L1_w = -tf.reduce_mean(tf.log(P_DQ))\n",
    "        L2_w = -tf.reduce_mean(tf.log(P_QD))\n",
    "        \n",
    "        ###sentence-level###\n",
    "        #heatmap\n",
    "        h_s = tf.nn.relu(tf.einsum('bj,cklj->bckl',e_s,v)) #pair-wise e_bar*v^T: ?x?xNx4\n",
    "        #attention\n",
    "        a_s = tf.einsum('bcjk,cjkl->bclk',h_s,v) #?x?xDx4 attnded visual reps for sen. for all pairs\n",
    "        #pair-wise score\n",
    "        a_s_norm = tf.nn.l2_normalize(a_s,axis=2)\n",
    "        e_s_norm = tf.nn.l2_normalize(e_s,axis=1)\n",
    "        R_sk = tf.einsum('bclk,bl->bck',a_s_norm,e_s_norm) #cosine for (sen,img_reps) for all pairs\n",
    "        R_s = tf.reduce_max(R_sk,axis=-1) #?x?\n",
    "        #posterior probabilities\n",
    "        P_DQ_s = tf.diag_part(tf.nn.softmax(gamma_2*R_s,axis=0)) #P(cap match img)\n",
    "        P_QD_s = tf.diag_part(tf.nn.softmax(gamma_2*R_s,axis=1)) #P(img match cap)\n",
    "        #losses\n",
    "        L1_s = -tf.reduce_mean(tf.log(P_DQ_s))\n",
    "        L2_s = -tf.reduce_mean(tf.log(P_QD_s))\n",
    "        #overall loss\n",
    "        loss = L1_w + L2_w + L1_s + L2_s\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "def attn(e_w,v,e_s):\n",
    "    ## Inputs: local and global cap and img features ##\n",
    "    ## Output: Heatmap for each word, Global Heatmap, Attnded Vis features, Corr-vals\n",
    "    #e: ?xTxD, v: ?xNx4xD, e_bar: ?xD\n",
    "    with tf.variable_scope('attention'):\n",
    "        ###word-level###\n",
    "        #heatmap pool\n",
    "        h = tf.nn.relu(tf.einsum('bij,bklj->bikl',e_w,v)) #pair-wise ev^T: ?xTxNx4\n",
    "        #attention\n",
    "        a = tf.einsum('bijk,bjkl->bilk',h,v) #?xTxDx4 attnded visual reps for each of T words\n",
    "        #pair-wise score\n",
    "        a_norm = tf.nn.l2_normalize(a,axis=2)\n",
    "        e_w_norm = tf.nn.l2_normalize(e_w,axis=2)\n",
    "        R_ik = tf.einsum('bilk,bil->bik',a_norm,e_w_norm) #cosine for T (words,img_reps) for all pairs\n",
    "        R_ik = tf.identity(R_ik,name='level_score_word')\n",
    "        R_i = tf.reduce_max(R_ik,axis=-1,name='score_word') #?xT\n",
    "        #R = tf.log(tf.pow(tf.reduce_sum(tf.exp(gamma_1*R_i),axis=1),1/gamma_1)) #? corrs\n",
    "        #heatmap\n",
    "        idx_i = tf.argmax(R_ik,axis=-1,name='level_index_word') #?xT index of the featuremap which maximizes R_i\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.histogram('histogram_w', idx_i)\n",
    "        ii,jj = tf.meshgrid(tf.range(tf.shape(idx_i)[0]),tf.range(tf.shape(idx_i)[1]),indexing='ij')\n",
    "        ii = tf.cast(ii,tf.int64)\n",
    "        jj = tf.cast(jj,tf.int64)\n",
    "        batch_idx_i = tf.stack([tf.reshape(ii,(-1,)),\n",
    "                                tf.reshape(jj,(-1,)),\n",
    "                                tf.reshape(idx_i,(-1,))],axis=1) #?Tx3 indices of argmax\n",
    "        N0=int(np.sqrt(h.get_shape().as_list()[2]))\n",
    "        h_max = tf.gather_nd(tf.transpose(h,[0,1,3,2]),batch_idx_i) #?TxN retrieving max heatmaps\n",
    "        heatmap_wd = tf.reshape(h_max,[tf.shape(h)[0],tf.shape(h)[1],N0,N0],name='heatmap_word')\n",
    "        heatmap_wd_l = tf.reshape(h,[tf.shape(h)[0],tf.shape(h)[1],N0,N0,tf.shape(h)[3]],name='level_heatmap_word')\n",
    "        \n",
    "        ###sentence-level###\n",
    "        #heatmap pool\n",
    "        h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx4\n",
    "        #attention\n",
    "        a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx4 attnded visual reps for sen.\n",
    "        #pair-wise score\n",
    "        a_s_norm = tf.nn.l2_normalize(a_s,axis=1)\n",
    "        e_s_norm = tf.nn.l2_normalize(e_s,axis=1)\n",
    "        R_sk = tf.einsum('bik,bi->bk',a_s_norm,e_s_norm) #cosine for (sen,img_reps)\n",
    "        R_sk = tf.identity(R_sk,name='level_score_sentence')\n",
    "        R_s = tf.reduce_mean(R_sk,axis=-1,name='score_sentence') #?\n",
    "        #heatmap\n",
    "        idx_k = tf.argmax(R_sk,axis=-1,name='level_index_sentence') #? index of the featuremap which maximizes R_i\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.histogram('histogram_s', idx_k)\n",
    "        ii_k = tf.cast(tf.range(tf.shape(idx_k)[0]),dtype='int64')\n",
    "        batch_idx_k = tf.stack([ii_k,idx_k],axis=1)\n",
    "        N0_g=int(np.sqrt(h_s.get_shape().as_list()[1]))\n",
    "        h_s_max = tf.gather_nd(tf.transpose(h_s,[0,2,1]),batch_idx_k) #?xN retrieving max heatmaps\n",
    "        heatmap_sd = tf.reshape(h_s_max,[-1,N0_g,N0_g],name='heatmap_sentence')\n",
    "        heatmap_sd_l = tf.reshape(h_s,[-1,N0_g,N0_g,tf.shape(h)[3]],name='level_heatmap_sentence')\n",
    "        \n",
    "    return heatmap_wd, heatmap_sd, R_i, R_s  \n",
    "\n",
    "def add_1by1_conv(feat_map,n_layers,n_filters,name,regularizer):\n",
    "    with tf.variable_scope(name+'_postConv'):\n",
    "        for i in range(n_layers):\n",
    "            with tf.variable_scope(name+'_stage_'+str(i)):\n",
    "                feat_map = tf.layers.conv2d(feat_map,filters=n_filters[i],kernel_size=[1,1],kernel_regularizer=regularizer)\n",
    "                feat_map = tf.nn.leaky_relu(feat_map,alpha=.25)\n",
    "    return feat_map\n",
    "\n",
    "def depth_selection(model):\n",
    "    with tf.variable_scope('stack_v'):\n",
    "        v1 = tf.identity(model['vgg_16/conv5/conv5_1'],name='v1')\n",
    "        v1 = add_1by1_conv(v1,n_layers=3,n_filters=[1024,1024,1024],name='v1',regularizer=regularizer)\n",
    "        size = v1.get_shape().as_list()[1:3]\n",
    "        resize_method = tf.image.ResizeMethod.BILINEAR\n",
    "        v2 = tf.identity(model['vgg_16/conv5/conv5_3'],name='v2')\n",
    "        #v2 = tf.image.resize_images(v2, size, method=resize_method)\n",
    "        v2 = add_1by1_conv(v2,n_layers=3,n_filters=[1024,1024,1024],name='v2',regularizer=regularizer)\n",
    "        v3 = tf.identity(model['vgg_16/conv4/conv4_1'],name='v3')\n",
    "        v3 = tf.image.resize_images(v3, size, method=resize_method, align_corners=True)\n",
    "        v3 = add_1by1_conv(v3,n_layers=3,n_filters=[1024,1024,1024],name='v3',regularizer=regularizer)\n",
    "        v4 = tf.identity(model['vgg_16/conv4/conv4_3'],name='v4')\n",
    "        v4 = tf.image.resize_images(v4, size, method=resize_method, align_corners=True)\n",
    "        v4 = add_1by1_conv(v4,n_layers=3,n_filters=[1024,1024,1024],name='v4',regularizer=regularizer)\n",
    "        v_all = tf.stack([v1,v2,v3,v4], axis=3)\n",
    "        v_all = tf.reshape(v_all,[-1,v_all.shape[1]*v_all.shape[2],v_all.shape[3],v_all.shape[4]])\n",
    "        v_all = tf.nn.l2_normalize(v_all, axis=-1, name='stacked_image_feature_maps')\n",
    "    return v_all\n",
    "\n",
    "def build_bilstm(w_embd,seq_length):\n",
    "    with tf.variable_scope('BiLSTM'):\n",
    "        # Forward direction cell\n",
    "        lstm_fw_cell = tf.contrib.rnn.LSTMCell(512, forget_bias=1.0)\n",
    "        # Backward direction cell\n",
    "        lstm_bw_cell = tf.contrib.rnn.LSTMCell(512, forget_bias=1.0)\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, w_embd, sequence_length = seq_length,\n",
    "                                                  dtype=tf.float32)\n",
    "    output = tf.concat(outputs,axis=2,name='BiLSTM_out')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "mode = tf.placeholder(tf.string, name='mode')\n",
    "isTraining = tf.equal(mode, 'train')\n",
    "regularizer = tf.contrib.layers.l2_regularizer(reg_val)\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    #building visual model\n",
    "    print('Building Visual Model...')\n",
    "    input_img = tf.placeholder(tf.float32, (None,299,299,3), name='input_img')\n",
    "    pre_processed_img = pre_process(input_img, 'vgg_preprocessing')\n",
    "    vis_model = pre_trained_load(model_name='vgg_16', image_shape=(None,299,299,3),\n",
    "                              input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=True)\n",
    "\n",
    "    v = depth_selection(vis_model) #(?,1225,4,1024)\n",
    "    \n",
    "   #building text model\n",
    "    print('Building Text Model...')\n",
    "    #sentence placeholder - list of sentences\n",
    "    text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "    #loading pre-trained ELMo\n",
    "    elmo = hub.Module(\"../modules/ELMo\", trainable=True)\n",
    "    #getting ELMo embeddings\n",
    "    elmo_embds = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "    w_embd = tf.identity(elmo_embds['word_emb'], name='elmo_word_embd') #?xTxD/2\n",
    "    lstm_embd = build_bilstm(w_embd,elmo_embds['sequence_len']) #?xTxD\n",
    "    #taking index of last word in each sentence\n",
    "    idx = elmo_embds['sequence_len']-1\n",
    "    batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1)\n",
    "    # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "    dim = lstm_embd.get_shape().as_list()[-1]\n",
    "    sen_embd = tf.concat([lstm_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    e_s = tf.layers.dense(sen_embd, units=1024)\n",
    "    e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "    e_s = tf.layers.dense(e_s, units=1024)\n",
    "    e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "    e_s = tf.nn.l2_normalize(e_s, axis=-1, name='sen_embedding')\n",
    "    \n",
    "    w_embd_tiled = tf.tile(w_embd,[1,1,2])\n",
    "    w_embd = tf.concat([tf.expand_dims(w_embd_tiled,axis=3),tf.expand_dims(lstm_embd,axis=3)],axis=3)\n",
    "    w_embd = tf.layers.dense(w_embd, units=1)[:,:,:,0]\n",
    "    e_w = tf.layers.dense(w_embd, units=1024)\n",
    "    e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "    e_w = tf.layers.dense(e_w, units=1024)\n",
    "    e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "    e_w = tf.nn.l2_normalize(e_w, axis=-1, name='w_embedding')\n",
    "    \n",
    "    heatmap_w,heatmap_s,R_i,R_s = attn(e_w,v,e_s)\n",
    "        \n",
    "loss = attn_loss(e_w,v,e_s) + tf.losses.get_regularization_loss()\n",
    "loss = tf.identity(loss, name='loss')\n",
    "        \n",
    "lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) - set(vis_model.model_weights))\n",
    "train_op = opt.minimize(loss, var_list=train_vars, name='train_op')\n",
    "\n",
    "global_saver = tf.train.Saver()\n",
    "    \n",
    "train_writer = tf.summary.FileWriter('./logs/vgg/vg', sess.graph)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'BiLSTM_VGG_VG'\n",
    "print('Initializing...')\n",
    "_ = sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#loading pretrained vgg weights\n",
    "print('Loading visual path model (vgg)...')\n",
    "vis_model.load_weights()\n",
    "    \n",
    "#loop on training data\n",
    "print('Start training...')\n",
    "iou_acc_w = np.zeros((n_epochs,))\n",
    "hit_acc_w = np.zeros((n_epochs,))\n",
    "iou_acc_s = np.zeros((n_epochs,))\n",
    "hit_acc_s = np.zeros((n_epochs,))\n",
    "train_loss = np.zeros((n_epochs,))\n",
    "lr_value_0 = .001\n",
    "max_val_iou = 0\n",
    "max_val_hit = 0\n",
    "for e in range(n_epochs):\n",
    "    print('\\n\\n=====Epoch: %d'%e)\n",
    "    avg_loss = 0\n",
    "    if e<9:\n",
    "        lr_value=lr_value_0\n",
    "    elif 9<=e<14:\n",
    "        lr_value=lr_value_0/2.0\n",
    "    elif e>=14:\n",
    "        lr_value=lr_value_0/4.0\n",
    "        \n",
    "    print('===Train')\n",
    "    for i in range(n_iter_per_epoch):\n",
    "        img_batch, cap_batch = batch_gen(ids_train, dict_train, txn)\n",
    "        feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value}\n",
    "        summary, loss_val, _ = sess.run([merged, loss, train_op], feed_dict)\n",
    "        if i%100==0:\n",
    "            train_writer.add_summary(summary, n_iter_per_epoch*e + i)\n",
    "        avg_loss+=loss_val\n",
    "        var = [i*n_batch, n_iter_per_epoch*n_batch, avg_loss/float(i+1)]\n",
    "        prnt = 'Sample {}/{}, train_loss:{:.4f} \\r'.format(var[0],var[1],var[2])\n",
    "        sys.stdout.write(prnt)                \n",
    "        sys.stdout.flush()     \n",
    "    train_loss[e] = avg_loss/float(n_iter_per_epoch+1)\n",
    "    \n",
    "    #validation phase\n",
    "    print('\\n===Validation')\n",
    "    iou_acc_w[e],hit_acc_w[e],iou_acc_s[e],hit_acc_s[e]=validate_referit(dict_val,ref_txn)\n",
    "    sv = 'Epoch:{}, Train_loss:{}, Val_iou_acc:{}, Val_hit_acc:{}\\r'.format(e,train_loss[e],iou_acc_w[e],hit_acc_w[e])\n",
    "    open('./logs/log_'+condition+'.txt', 'w').write(sv)\n",
    "    if hit_acc_s[e]>max_val_hit:\n",
    "        max_val_hit = hit_acc_s[e]\n",
    "        print('\\nHit accuracy improved. Saving best model...\\r')\n",
    "        global_saver.save(sess, '../saved_models/model_'+condition+'_best_hit')\n",
    "    if iou_acc_s[e]>max_val_iou:\n",
    "        max_val_iou = iou_acc_s[e]\n",
    "        print('\\nIoU accuracy improved. Saving best model...\\r')\n",
    "        global_saver.save(sess, '../saved_models/model_'+condition+'_best_iou')\n",
    "        \n",
    "print('\\n\\nTraining done.')\n",
    "#saving the session\n",
    "print('Saving model...')\n",
    "global_saver.save(sess, '../saved_models/model_'+condition)\n",
    "print('Saving done.')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='Train loss '+condition)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(100.*iou_acc_w, label='Validation iou_acc_w '+condition)\n",
    "plt.plot(100.*hit_acc_w, label='Validation hit_acc_w '+condition)\n",
    "plt.plot(100.*iou_acc_s, label='Validation iou_acc_s '+condition)\n",
    "plt.plot(100.*hit_acc_s, label='Validation hit_acc_s '+condition)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
